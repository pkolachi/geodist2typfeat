{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pkolachi/geodist2typfeat/blob/master/exptnbs/sigtyp-st2020-part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nuSUXKKMhvlL"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "Using matplotlib backend: MacOSX\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Library/Python/3.7/lib/python/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random', 'deprecated']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "%matplotlib inline\n",
    "%pylab\n",
    "\n",
    "fpurl   = 'https://raw.githubusercontent.com/sigtyp/ST2020/master/data/train.csv'\n",
    "# the header from the csv is not properly tab-seperated. hence hard-coding\n",
    "header  = ['wals_code', 'name', \n",
    "           'latitude', 'longitude', \n",
    "           'genus', 'family', 'countrycodes', \n",
    "           'features'\n",
    "          ]\n",
    "\n",
    "CVFOLDS = 2   # default: 2 folds\n",
    "N = -1        # default: use all samples \n",
    "K = 10        # default: use only 5 feature classes\n",
    "REPEAT  = -1\n",
    "# turn this on iff running from command-line to test performance across \n",
    "# different values for (CVFOLDS, K, REPEAT) \n",
    "BATCH = False  \n",
    "\n",
    "import itertools as it\n",
    "from collections import Counter, defaultdict\n",
    "from operator    import itemgetter\n",
    "from IPython.display import display as pd_displayHTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OXxOiiVAmiSW"
   },
   "outputs": [],
   "source": [
    "#%pip install -q pycodestyle_magic flake8\n",
    "#%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_kKY1tspATV"
   },
   "outputs": [],
   "source": [
    "#%flake8_on -m 119 --ignore=E111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ka9wrCHqzQ7J"
   },
   "source": [
    "I hoped the provided train/test dataset is CSV compliant so that loading the dataset is as simple as using *pandas.read_csv*. It turned out not to be the case. The problem is with the header in the provided csv file, which makes inferring the columns using *header=auto* impossible. This is easily handled by hard-coding the column names in the header and skipping the first row when using *pandas.read_csv*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1o-kKUMp7HD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "(1125, 7) (1125,) (1125, 185) (973, 2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "%pip install -q --user pandas==1.0.3\n",
    "import pandas as pd\n",
    "df = pd.read_csv(fpurl, sep='\\t', header=None, names=header,\n",
    "                 #index_col=0,\n",
    "                 error_bad_lines=True, skiprows=[0])\n",
    "\"\"\"\n",
    "# since this pynb will never be run on the held-out test set\n",
    "if CVFOLDS <= 1:\n",
    "  trnS, tstS = 0, 0   # dummy values for sizes of train and test partitions\n",
    "else:\n",
    "  tstdf = pd.read_csv(tstfpurl, sep='\\t', header=None, names=header,\n",
    "                      error_bad_lines=True, skiprows=[0])\n",
    "  trnS, tstS = df.shape[0], tstdf.shape[0]\n",
    "  df.append(tstdf)\n",
    "\"\"\"\n",
    "missValue = '*-missing-*'\n",
    "featsFull = df.iloc[:, 0:-1]\n",
    "clablFull = df.iloc[:, -1]\n",
    "alablInst = Counter(albl for inst in clablFull for albl in inst.split('|'))\n",
    "alablTabl = pd.DataFrame([{'name': n, 'id': i, 'freq': f}\n",
    "                          for i,(n,f) in enumerate(alablInst.most_common(), start=1)\n",
    "                         ]).set_index('name')\n",
    "alablFull = pd.DataFrame([dict(albl.split('=', 1) for albl in inst.split('|'))\n",
    "                          for inst in clablFull\n",
    "                         ]).fillna(missValue) # fill missing values (no NaN) \n",
    "for incol in ['wals_code', 'name', 'genus', 'family', 'countrycodes']:\n",
    "  featsFull[incol] = featsFull[incol].astype('category')\n",
    "clablFull = clablFull.astype('category')\n",
    "alablFull = alablFull.astype('category')\n",
    "\n",
    "print(featsFull.shape, clablFull.shape, alablFull.shape, alablTabl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding manual features\n",
    "\n",
    "### Ideas\n",
    "+    Convert latitude and longitude to UTM which can be encoded as a discrete feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s49Vl3rxLmjL"
   },
   "outputs": [],
   "source": [
    "# Adding manual features\n",
    "# Ideas\n",
    "## 1. Convert latitude and longitude to UTM which can be encoded as a discrete feature \n",
    "###### -- wiki description https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system\n",
    "###### -- found source code to use: https://github.com/Turbo87/utm\n",
    "###### -- kaggle question on similar topic https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/discussion/62711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JV9tca5Ay0ks"
   },
   "source": [
    "Let's plot a few simple statistics about the dataset. \n",
    "1.   Histogram of the complex labels in the dataset\n",
    "2.   Scatterplots of genus vs labels, family vs labels and countrycodes vs labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1p-kfcpy0Yc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --user seaborn==0.10.0\n",
    "import seaborn as sns\n",
    "def plot_datastats(features, clabels, alabels):\n",
    "  return\n",
    "\n",
    "if not BATCH:\n",
    "  plot_datastats(featsFull, clablFull, alablFull)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hS-ZOx0MZ32p"
   },
   "source": [
    "The dataset is loaded into a DataFrame and seperated into two parts: input features and output labels. \n",
    "\n",
    "We know a few things about the input features like what are categorical features and what are numerical features. So, we encode the different columns in the feats DataFrame accordingly. *Hopefully this matters* when training different classifiers (especially thinking of decision trees). \n",
    "\n",
    "At this point, I'm not looking at best encoding scheme for the labels which are composite labels themselves (more on this later). The training dataset provided has 1109 unique labels for the dataset of 1125 languages. This indicates that there is *an optimal representation* for the label set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5pxmCRll3lMr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125, 7) (1125,) (1125, 185) (1125, 10)\n"
     ]
    }
   ],
   "source": [
    "# sub-select data frame to speed-up experiments while debugging\n",
    "import random\n",
    "# because we want sampling without replacement when we work with selected\n",
    "# features classes to test, using random makes statistics across runs\n",
    "# incomparable -- so use a uniform distribution to select feature classes for\n",
    "# comparison across different experiments.\n",
    "# to get robust estimates while testing, use random selection\n",
    "if N < 2 or N > featsFull.shape[0]:\n",
    "  subsid = list(range(featsFull.shape[0]))\n",
    "else:\n",
    "  subsid = list(range(0, featsFull.shape[0], featsFull.shape[0]//N))[:N]\n",
    "\n",
    "if K < 0 or K > alablFull.shape[1]:\n",
    "  subfci = list(range(alablFull.shape[1]))\n",
    "else:\n",
    "  subfci = list(sorted(random.sample(range(alablFull.shape[1]), K)))\n",
    "  #subfci = list(range(0, alablFull.shape[1], alablFull.shape[1]//K))[:K])\n",
    "subfcs = list(alablFull.columns[i] for i in subfci)\n",
    "\n",
    "#relevfeats = [i for i,f in enumerate(header)][2:-1]\n",
    "featsFull_ = featsFull.iloc[subsid, :]\n",
    "clablFull_ = clablFull.iloc[subsid]\n",
    "alablSub_  = alablFull.iloc[subsid, subfci]\n",
    "alablFull_ = alablFull.iloc[subsid, :]\n",
    "\n",
    "print(featsFull_.shape, clablFull_.shape, alablFull_.shape, alablSub_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I28aYTkvm6TI"
   },
   "source": [
    "Let us try a few classifiers using *scikit-learn* at this point. \n",
    "\n",
    "For what it is worth, the accuracies can be worse than a coin flip, considering the sparse label set.\n",
    "\n",
    "\n",
    " features of languages spoken in close proximity and belonging to the same family should be highly informative in predicting the typographical features for a new language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJZ0u8vkZ3XB"
   },
   "outputs": [],
   "source": [
    "deprecated = \"\"\"\n",
    "# it is essential to make deep copies of the frame when building numpy\n",
    "# matrices used for classification experiments.\n",
    "# if not, changes to the matrix representations e.g. encoding categorial\n",
    "# variables as ordinals or sparse-matrices are reflected in the original frame\n",
    "# which results in errors when trying to re-use the frames for other experiments\n",
    "# e.g. lookup in the atomic-label table built above results in errors because\n",
    "# the lookup tries to find fnc=lbl-idx where idx is the category code\n",
    "X   = featsFull_.copy(deep=False)\n",
    "ccs = X.select_dtypes(['category']).columns\n",
    "X[ccs] = X[ccs].apply(lambda x: x.cat.codes)\n",
    "\n",
    "Y = clablFull_.copy(deep=False).cat.codes\n",
    "\n",
    "Y_  = alablFull_.copy(deep=False)\n",
    "ccs = Y_.select_dtypes(['category']).columns\n",
    "Y_[ccs] = Y_[ccs].apply(lambda x: x.cat.codes)\n",
    "\n",
    "subY_ = alablSub_.copy(deep=False)\n",
    "ccs = subY_.select_dtypes(['category']).columns\n",
    "subY_[ccs] = subY_[ccs].apply(lambda x: x.cat.codes)\n",
    "\n",
    "X  = X.to_numpy()\n",
    "Y  = Y.to_numpy()\n",
    "Y_ = Y_.to_numpy()\n",
    "subY_ = subY_.to_numpy()\n",
    "\n",
    "print(X.shape, Y.shape, Y_.shape, subY_.shape)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kadqK9JkhKKw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "(1125,) (1125, 185) (1125, 973)\n"
     ]
    }
   ],
   "source": [
    " %pip install -q --user scikit-learn==0.22.2.post1\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder \n",
    "\n",
    "lblenc = LabelEncoder().fit(clablFull_)\n",
    "Ynms   = lblenc.classes_\n",
    "Y      = lblenc.transform(clablFull_)\n",
    "\n",
    "lblenc = OrdinalEncoder().fit(alablFull_)\n",
    "aYnms  = lblenc.categories_\n",
    "Y_     = lblenc.transform(alablFull_)\n",
    "\n",
    "mlablFull = [[alablTabl.loc['{0}={1}'.format(fcn, lbl),'id']\n",
    "              for fcn,lbl in row.items() if lbl != missValue]\n",
    "             for row in alablFull_.to_dict(orient='records')\n",
    "            ]\n",
    "Ymlbl = MultiLabelBinarizer().fit_transform(mlablFull)\n",
    "\n",
    "rawX = featsFull_.copy(deep=False)\n",
    "\n",
    "print(Y.shape, Y_.shape, Ymlbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JBPJvqvDNJcj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1125,) (1125, 10) (1125, 49)\n"
     ]
    }
   ],
   "source": [
    "if len(subfci) < alablFull_.shape[1]:\n",
    "  clablSub = ['|'.join('{0}={1}'.format(fcn, lbl) for fcn, lbl in row.items()\n",
    "                       if lbl != missValue)\n",
    "              for row in alablSub_.to_dict(orient='records')\n",
    "             ]\n",
    "  lblenc   = LabelEncoder().fit(clablSub)\n",
    "  subYnms  = lblenc.classes_\n",
    "  subY     = lblenc.transform(clablSub)\n",
    "\n",
    "  lblenc   = OrdinalEncoder().fit(alablSub_)\n",
    "  subaYnms = lblenc.categories_\n",
    "  subY_    = lblenc.transform(alablSub_)\n",
    "  \n",
    "  mlablSub = [[alablTabl.loc['{0}={1}'.format(fcn, lbl),'id']\n",
    "               for fcn,lbl in row.items() if lbl != missValue]\n",
    "              for row in alablSub_.to_dict(orient='records')\n",
    "             ]\n",
    "  subYmlbl = MultiLabelBinarizer().fit_transform(mlablSub)\n",
    "else:\n",
    "  subY, subY_, subYmlbl = Y, Y_, Ymlbl\n",
    "\n",
    "print(subY.shape, subY_.shape, subYmlbl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzMTKIUHWNko"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder\n",
    "\n",
    "from sklearn import pipeline as skpipe\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder \n",
    "\n",
    "numfxr = SimpleImputer(strategy='median')\n",
    "strfxr = SimpleImputer(strategy='constant', fill_value='-empty-')\n",
    "strfx_ = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "stdtrn = StandardScaler()\n",
    "ohetrn = OneHotEncoder(handle_unknown='ignore', sparse=True)\n",
    "ordtrn = OrdinalEncoder(categories='auto')\n",
    "\n",
    "# for numerical features, fill unknown feature values with median value and \n",
    "# scale the column with mean and variance\n",
    "numfeats = ['latitude', 'longitude']\n",
    "numtrans = skpipe.Pipeline(steps=[('imputer', numfxr), ('transform',  stdtrn)])\n",
    "\n",
    "# for categorial features, try both one-hot encoding and ordinal encoding\n",
    "catfeats = ['wals_code', 'name', 'genus', 'family', 'countrycodes'] \n",
    "ohcattrans = skpipe.Pipeline(steps=[('imputer', strfxr), ('transform', ohetrn)])\n",
    "ohtrans = ColumnTransformer(transformers=[('num', numtrans, numfeats), \n",
    "                                          ('cat', ohcattrans, catfeats)])\n",
    "\n",
    "oecattrans = skpipe.Pipeline(steps=[('imputer', strfxr), ('transform', ordtrn)])\n",
    "oetrans = ColumnTransformer(transformers=[('num', numtrans, numfeats), \n",
    "                                          ('cat', oecattrans, catfeats)])\n",
    "# There are known issues in sklearn when integrating OrdinalFeatures into the pipeline\n",
    "# that cause problems with the standard/usual way of incorporating a transformer\n",
    "# to extract ordinal features. \n",
    "# Below is a 'hacky' version that gets around this and simulates the use of \n",
    "# dense numerical features for this classification task -- note that, this is\n",
    "# still a hack\n",
    "uniqcats = [rawX.loc[:,catn].unique() for catn in catfeats]\n",
    "oetrans_ = skpipe.Pipeline(steps=[('imputer', strfx_),\n",
    "                                  ('transform', OrdinalEncoder(categories=uniqcats))])\n",
    "altoetrn = ColumnTransformer(transformers=[('num', numtrans, numfeats), \n",
    "                                           ('cat', oetrans_, catfeats)])\n",
    "\n",
    "preprocessors = {'ohe': ohtrans,\n",
    "                #'ord': oetrans,\n",
    "                 'ord': altoetrn,\n",
    "                }\n",
    "\n",
    "middlelayer = {'dim0': 'passthrough',\n",
    "              #'pca':  PCA(svd_solver='arpack', random_state=20200408),\n",
    "               'svd':  TruncatedSVD(algorithm='arpack', random_state=20200408),\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8GbLxqzoGwwL"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# all these classifiers support multi-class classification in sklearn\n",
    "mclsclfnms = ['knn', 'lsvm', 'svc', 'dt', 'rf', 'mlp', 'adb', \n",
    "              'nb', 'ridge', 'dumbase',\n",
    "             #'gp', 'qda', \n",
    "             ]\n",
    "mclsclfobj = [KNeighborsClassifier(p=1), # works well for all inputs \n",
    "              LinearSVC(penalty='l1', dual=False, C=0.01, random_state=20200408),\n",
    "              SVC(gamma=2, random_state=20200408),\n",
    "              DecisionTreeClassifier(random_state=20200408),\n",
    "              RandomForestClassifier(random_state=20200408),\n",
    "              MLPClassifier(random_state=20200408),\n",
    "              AdaBoostClassifier(random_state=20200408),\n",
    "              GaussianNB(),\n",
    "              RidgeClassifier(random_state=20200408),\n",
    "              DummyClassifier(strategy=\"most_frequent\"),\n",
    "              GaussianProcessClassifier(),\n",
    "              QuadraticDiscriminantAnalysis(),\n",
    "             ]\n",
    "mclsclfopt = dict(zip(mclsclfnms, mclsclfobj))\n",
    "\n",
    "# setup pipelines for combination of preprocessors and classifiers\n",
    "mclsnullcs = []  #[('ord', 'lsvm'), ('ohe', 'mlp'), ('ord', 'dumbase')]\n",
    "pipelines  = {(clf, enc, dim):\n",
    "               skpipe.make_pipeline(preprocessors[enc], middlelayer[dim],\n",
    "                                    mclsclfopt[clf])\n",
    "              for clf in mclsclfopt\n",
    "              for enc in preprocessors\n",
    "              for dim in middlelayer\n",
    "              if (enc, clf) not in mclsnullcs\n",
    "             }\n",
    "pipelines  = dict(sorted(pipelines.items()))\n",
    "\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "REPEAT  = REPEAT  if REPEAT  >  1 else 1  # sanity-check\n",
    "CVFOLDS = CVFOLDS if CVFOLDS >= 2 else 2  # sanity-check\n",
    "# scikit-learn documentation recommends using StratifiedKFold for classification\n",
    "# problems to preserve class balance across folds. however, in this case, \n",
    "# we use KFold and RepeatedKFold because \n",
    "#  number of items in a class <= CVFOLDS (works only with 2 folds for entire dataset)\n",
    "#  there is not much balance to preserve w.r.t. complex labels\n",
    "cvsplits = list(RepeatedKFold(n_splits=CVFOLDS, \n",
    "                              n_repeats=REPEAT, random_state=20200408\n",
    "                             ).split(rawX, Y))\n",
    "\n",
    "statnames = ['classifier', \n",
    "             'avg.acc/tst', 'avg.acc/trn', 'std.acc/tst', 'std.acc/trn', \n",
    "             'avg.time/prd', 'avg.time/trn'] \n",
    "statcodes = ['clfn', 'mtsts', 'mtrns', 'vtsts', 'vtrns', 'predt', 'trint']\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-fix-futurewarning-messages-in-scikit-learn/\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
    "simplefilter(action='ignore', category=ConvergenceWarning)\n",
    "simplefilter(action='ignore', category=FitFailedWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-qdcL_F4cmi"
   },
   "outputs": [],
   "source": [
    "def plot_accuracies(accuracies, savefile=None):\n",
    "  hastime = 'avg.time/pred' in accuracies.columns or 'avg.time/trn' in accuracies.columns\n",
    "  fig, axs = plt.subplots(2, 2, sharex=True, sharey='row', figsize=(12, 8))\n",
    "  axs[0,0].bar(accuracies['classifier'], accuracies['avg.acc/tst'],\n",
    "               yerr=accuracies['std.acc/tst'])\n",
    "  axs[0,0].text(.5, .9, 'held-out', horizontalalignment='center', \n",
    "                transform=axs[0,0].transAxes)\n",
    "  axs[0,1].bar(accuracies['classifier'], accuracies['avg.acc/trn'],\n",
    "               yerr=accuracies['std.acc/trn'])\n",
    "  axs[0,1].text(.5, .9, 'training', horizontalalignment='center', \n",
    "                transform=axs[0,1].transAxes)\n",
    "  if hastime:\n",
    "    axs[1,0].bar(accuracies['classifier'], accuracies['avg.time/prd'])\n",
    "    axs[1,0].text(.5, .9, 'prediction', horizontalalignment='center', \n",
    "                  transform=axs[1,0].transAxes)\n",
    "    axs[1,1].bar(accuracies['classifier'], accuracies['avg.time/trn'])\n",
    "    axs[1,1].text(.5, .9, 'training', horizontalalignment='center', \n",
    "                  transform=axs[1,1].transAxes)\n",
    "\n",
    "  axs[0,0].set_ylim(ymin=0)\n",
    "  axs[1,0].set_ylim(ymin=0)\n",
    "  axs[0,0].set_ylabel('Accuracy')\n",
    "  axs[1,0].set_ylabel('Time(s)')\n",
    "  # rotate all xtick labels\n",
    "  _ = [l.set_rotation(90) for a in axs.flatten() for l in a.get_xticklabels()]\n",
    "  # minimize space between subplots\n",
    "  plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "  if not savefile:\n",
    "    plt.show()\n",
    "  else:\n",
    "    plt.savefig(savefile)\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWvm2I-2oPwg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-29-1e51870963d9>\", line 20, in <module>\n",
      "    clfaccs = trainFullClassifiersCV(pipelines, rawX, Y)\n",
      "  File \"<ipython-input-29-1e51870963d9>\", line 10, in trainFullClassifiersCV\n",
      "    return_train_score=True\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 236, in cross_validate\n",
      "    for train, test in cv.split(X, y, groups))\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/joblib/parallel.py\", line 924, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/joblib/parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/joblib/parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/joblib/_parallel_backends.py\", line 182, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/joblib/_parallel_backends.py\", line 549, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/joblib/parallel.py\", line 225, in __call__\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/joblib/parallel.py\", line 225, in <listcomp>\n",
      "    for func, args, kwargs in self.items]\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 547, in _fit_and_score\n",
      "    train_scores = _score(estimator, X_train, y_train, scorer)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 591, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 89, in __call__\n",
      "    score = scorer(estimator, *args, **kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 371, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 116, in <lambda>\n",
      "    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/pipeline.py\", line 619, in score\n",
      "    return self.steps[-1][-1].score(Xt, y, **score_params)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/base.py\", line 369, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/svm/_base.py\", line 594, in predict\n",
      "    y = super().predict(X)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/svm/_base.py\", line 317, in predict\n",
      "    return predict(X)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/svm/_base.py\", line 339, in _dense_predict\n",
      "    cache_size=self.cache_size)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/admin/Library/Python/3.7/lib/python/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/local/Cellar/python/3.7.6_1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection as skms\n",
    "\n",
    "def codify_classifier(clf):\n",
    "  return clf if isinstance(clf, str) else '/'.join(clf)\n",
    "\n",
    "def trainFullClassifiersCV(classifiers, X, Y):\n",
    "  clfaccs = []\n",
    "  for iclf, nclf in enumerate(classifiers):\n",
    "    clfsce = skms.cross_validate(classifiers[nclf], X, Y, cv=cvsplits, \n",
    "                                 return_train_score=True\n",
    "                                )\n",
    "    clfnfo = [codify_classifier(nclf), \n",
    "              100*clfsce['test_score'].mean(), 100*clfsce['train_score'].mean(),\n",
    "              100*clfsce['test_score'].std(),  100*clfsce['train_score'].std(),\n",
    "              clfsce['score_time'].sum(), clfsce['fit_time'].sum()\n",
    "             ]\n",
    "    clfaccs.append(dict(zip(statnames, clfnfo)))\n",
    "  return pd.DataFrame(clfaccs)\n",
    "\n",
    "clfaccs = trainFullClassifiersCV(pipelines, rawX, Y)\n",
    "if not BATCH:\n",
    "  clfaccs_ = clfaccs.dropna()\n",
    "  plot_accuracies(clfaccs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvX9mw94saLx"
   },
   "outputs": [],
   "source": [
    "%time sclfaccs = trainFullClassifiersCV(pipelines, rawX, subY)\n",
    "if not BATCH:\n",
    "  sclfaccs_ = sclfaccs.dropna()\n",
    "  pd_displayHTML(sclfaccs_.style.hide_index())\n",
    "  plot_accuracies(sclfaccs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbptkdD5ka6L"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "paramsKNN = {\n",
    "             'kneighborsclassifier__n_neighbors':range(3, 11), \n",
    "             'kneighborsclassifier__weights':('uniform', 'distance'),\n",
    "            #'kneighborsclassifier__algorithm':('auto', 'ball_tree', 'kd_tree', 'brute'),\n",
    "             'kneighborsclassifier__p':(1, 2), \n",
    "            }\n",
    "paramsSVL = {\n",
    "            #'linearsvc__penalty':('l1', 'l2'),\n",
    "            #'linearsvc__loss':('squared_hinge', 'hinge'), \n",
    "            #'linearsvc__dual':(False, True), \n",
    "            #'linearsvc__tol',     \n",
    "             'linearsvc__C':np.arange(0, 0.25, 0.01),\n",
    "            #'linearsvc__fit_intercept', \n",
    "            #'linearsvc__intercept_scaling', \n",
    "            #'linearsvc__class_weight':(None, 'balanced'), \n",
    "            #'linearsvc__max_iter', \n",
    "            }\n",
    "paramsSVC = {\n",
    "             'svc__C':10**-np.arange(-1, 2.5, 0.35), \n",
    "             'svc__kernel':('poly', 'rbf', 'sigmoid',), \n",
    "             'svc__degree':range(2, 5), \n",
    "            #'svc__gamma':np.arange(0, 4, 0.5), \n",
    "            #'svc__coef0', \n",
    "            #'svc__shrinking', \n",
    "            #'svc__tol', \n",
    "             'svc__class_weight':(None, 'balanced'), \n",
    "            }\n",
    "paramsMLP = {\n",
    "            #'mlpclassifier__hidden_layer_sizes':[(size,) for size in range(50, 60, 20)], \n",
    "            #'mlpclassifier__activation:('relu', 'logistic', 'tanh'), \n",
    "            #'mlpclassifier__solver:('adam', 'lbfgs'), \n",
    "             'mlpclassifier__alpha':10.0**np.arange(-7, 4), \n",
    "            #'mlpclassifier__max_iter':range(200, 1001, 100),\n",
    "            #'mlpclassifier__early_stopping':(True, False),\n",
    "            }  \n",
    "paramsDT  = {\n",
    "             'decisiontreeclassifier__criterion':('gini', 'entropy'), \n",
    "             'decisiontreeclassifier__max_features':('auto', 'sqrt', 'log2', None), \n",
    "             'decisiontreeclassifier__class_weight':(None, 'balanced'),\n",
    "             'decisiontreeclassifier__ccp_alpha':[i/10 for i in range(0, 11)], \n",
    "            #'decisiontreeclassifier__max_depth':, \n",
    "            #'decisiontreeclassifier__max_leaf_nodes':, \n",
    "            #'decisiontreeclassifier__min_impurity_decrease',\n",
    "            #'decisiontreeclassifier__min_impurity_split', \n",
    "            #'decisiontreeclassifier__min_samples_leaf', \n",
    "            #'decisiontreeclassifier__min_samples_split', \n",
    "            #'decisiontreeclassifier__min_weight_fraction_leaf', \n",
    "            #'decisiontreeclassifier__presort', \n",
    "            #'decisiontreeclassifier__random_state', \n",
    "            #'decisiontreeclassifier__splitter'\n",
    "            }\n",
    "paramsRF  = {\n",
    "             'randomforestclassifier__n_estimators':range(10, 201, 10),\n",
    "            #'randomforestclassifier__criterion':('gini', 'entropy'),\n",
    "            #'randomforestclassifier__max_features':('auto', 'sqrt', 'log2', None),\n",
    "            #'randomforestclassifier__bootstrap':(True, False),\n",
    "            #'randomforestclassifier__class_weight':(None, 'balanced'), \n",
    "            #'randomforestclassifier__ccp_alpha':[i/10 for i in range(0, 11)],\n",
    "            #'randomforestclassifier__max_depth', \n",
    "            #'randomforestclassifier__min_samples_split', \n",
    "            #'randomforestclassifier__min_samples_leaf', \n",
    "            #'randomforestclassifier__min_weight_fraction_leaf',\n",
    "            #'randomforestclassifier__max_leaf_nodes', \n",
    "            #'randomforestclassifier__min_impurity_decrease', \n",
    "            #'randomforestclassifier__min_impurity_split', \n",
    "            #'randomforestclassifier__max_samples', \n",
    "            }\n",
    "paramsDummy = {'dummyclassifier__strategy':('stratified', 'most_frequent', 'prior', 'uniform'),\n",
    "              #'dummyclassifier__constant':(1), \n",
    "              #'dummyclassifier__random_state', \n",
    "              }\n",
    "\n",
    "#print(pipelines[('lsvm', 'ord', 'svd')].get_params().keys())\n",
    "\n",
    "cmnchoices = set()\n",
    "for clf in (_ for _ in pipelines if _[0] == 'svc'):\n",
    "  if clf[1:] == ('ohe', 'pca'):\n",
    "    continue\n",
    "  grdmclsclf = GridSearchCV(pipelines[clf], param_grid=paramsSVC, \n",
    "                            cv=cvsplits).fit(rawX, subY)\n",
    "  bstprmsstr = '|'.join(\"{0}={1}\".format(k,v) for k,v in grdmclsclf.best_params_.items())\n",
    "  print(codify_classifier(clf), grdmclsclf.best_score_, bstprmsstr)\n",
    "  cmnchoices = cmnchoices.intersection(grdmclsclf.best_params_.items()) if len(cmnchoices) else set(grdmclsclf.best_params_.items())\n",
    "  grdmclsres = pd.DataFrame(grdmclsclf.cv_results_)\n",
    "print(cmnchoices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3IC4j3K0_SW"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection as skms\n",
    "\n",
    "def trainIndClassifiersCV(classifiers, X, matY, return_clfinsts=False):\n",
    "  lclfinst  = {}  # table to store classifiers for later use\n",
    "  lclfaccs = np.zeros((matY.shape[-1], len(classifiers), 6))\n",
    "  avgcaccs = []\n",
    "  for iclf, nclf in enumerate(classifiers):\n",
    "    for indY in range(matY.shape[-1]):\n",
    "      clfsce = skms.cross_validate(classifiers[nclf], X, matY[:,indY], cv=cvsplits,\n",
    "                                   return_train_score=True, \n",
    "                                   return_estimator=return_clfinsts\n",
    "                                  )\n",
    "      lclfaccs[indY][iclf] = [100*clfsce['test_score'].mean(), \n",
    "                              100*clfsce['train_score'].mean(), \n",
    "                              100*clfsce['test_score'].std(),  \n",
    "                              100*clfsce['train_score'].std(), \n",
    "                              clfsce['score_time'].sum(), \n",
    "                              clfsce['fit_time'].sum()\n",
    "                             ]\n",
    "      if return_clfinsts:\n",
    "        lclfinst[(nclf, indY)] = clfsce['estimator']\n",
    "    clfnfo = [codify_classifier(nclf), \n",
    "              lclfaccs[:,iclf,0].mean(), lclfaccs[:,iclf,1].mean(),\n",
    "              lclfaccs[:,iclf,2].mean(), lclfaccs[:,iclf,3].mean(),\n",
    "              lclfaccs[:,iclf,4].sum(),  lclfaccs[:,iclf,5].sum()\n",
    "             ]\n",
    "    avgcaccs.append(dict(zip(statnames, clfnfo)))\n",
    "  if return_clfinsts:\n",
    "    return (pd.DataFrame(avgcaccs), lclfinst)\n",
    "  else:\n",
    "    return pd.DataFrame(avgcaccs)\n",
    "\n",
    "%time avgcaccs, lclclfs = trainIndClassifiersCV(pipelines, rawX, subY_, return_clfinsts=True)\n",
    "if not BATCH:\n",
    "  avgcaccs_ = avgcaccs.dropna()\n",
    "  #print(avgcaccs_.round(3).to_markdown(showindex=False))\n",
    "  pd_displayHTML(avgcaccs_.style.hide_index())\n",
    "  plot_accuracies(avgcaccs_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXQ4O5uXkZHZ"
   },
   "outputs": [],
   "source": [
    "from sklearn import model_selection as skms\n",
    "from sklearn import metrics as skmt\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "def skmt_mlmc_accuracy_score(y_true, y_pred):\n",
    "  \"Classification accuracy for multi-label multi-class problems\"\n",
    "  n_samples = y_true.shape[0]\n",
    "  return sum(1.0 if np.array_equal(y_true[i], y_pred[i]) else 0\n",
    "             for i in range(n_samples)\n",
    "            ) / n_samples\n",
    "\n",
    "def jntTestIndClassifiersCV(classifiers, X, matY, clfinstances=None):\n",
    "  if not clfinstances:\n",
    "    _, clfinstances = trainIndClassifiersCV(classifiers, X, matY, return_clfinsts=True)\n",
    "  trnpids = list(map(itemgetter(0), cvsplits))\n",
    "  tstpids = list(map(itemgetter(1), cvsplits))\n",
    "  # when passing the numerical matrix directly to the classifier\n",
    "  _predsst = lambda clf,sids: clf.predict(X[sids,:]).reshape((-1, 1))\n",
    "  # we are now handling the DataFrame directly using pipelines\n",
    "  predsst = lambda clf, sids: clf.predict(X.iloc[sids,:]).reshape((-1, 1))\n",
    "  jclfaccs = []\n",
    "  for iclf, nclf in enumerate(classifiers):\n",
    "    try:\n",
    "      tstpreds, trnpreds = [], []\n",
    "      for cvid, (trnids,tstids) in enumerate(cvsplits):\n",
    "        indpreds = list(it.starmap(predsst, [(clfinstances[nclf, indY][cvid], tstids)\n",
    "                                             for indY in range(matY.shape[-1])]))\n",
    "        tstpreds.append(np.hstack(indpreds))\n",
    "        indpreds = list(it.starmap(predsst, [(clfinstances[nclf, indY][cvid], trnids)\n",
    "                                             for indY in range(matY.shape[-1])]))  \n",
    "        trnpreds.append(np.hstack(indpreds))\n",
    "        \n",
    "      tstaccs = 100*np.array([skmt_mlmc_accuracy_score(matY[sids], preds)\n",
    "                              for sids, preds in zip(tstpids, tstpreds)])\n",
    "      trnaccs = 100*np.array([skmt_mlmc_accuracy_score(matY[sids], preds)\n",
    "                              for sids, preds in zip(trnpids, trnpreds)])\n",
    "      clfnfo = [codify_classifier(nclf), \n",
    "                tstaccs.mean(), trnaccs.mean(), tstaccs.std(), trnaccs.std()]\n",
    "      jclfaccs.append(dict(zip(statnames, clfnfo)))\n",
    "    except NotFittedError as err:\n",
    "      continue\n",
    "  return pd.DataFrame(jclfaccs)\n",
    "\n",
    "%time jclfaccs = jntTestIndClassifiersCV(pipelines, rawX, subY_, clfinstances=lclclfs)\n",
    "if not BATCH:\n",
    "  jclfaccs_ = jclfaccs.dropna()\n",
    "  pd_displayHTML(jclfaccs_.style.hide_index())\n",
    "  plot_accuracies(jclfaccs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRiNDJVKLrR6"
   },
   "source": [
    "\n",
    "TODO: \n",
    "\n",
    "*   hyper-parameter search and pick 3 classifiers\n",
    "*   also pick an optimal encoding scheme\n",
    "*   test some manual feature additions like geographic distances\n",
    "*   also check l1 regularization and see which features matter the most\n",
    "*   DT/RF & MLP seem to have non-convex optimizations or some other random seed initialization. can't replicate results when run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3eFDaf8sy-b"
   },
   "outputs": [],
   "source": [
    "mlblclfnms = ['1knn', '2knn', 'mlp', 'dt', 'rf', 'dumbase']\n",
    "mlblclfobj = [KNeighborsClassifier(p=1), # Manhattan distance\n",
    "              KNeighborsClassifier(p=2), # Euclidean distance\n",
    "              MLPClassifier(activation='logistic', solver='lbfgs', alpha=10, \n",
    "                           early_stopping=True, random_state=20200408),\n",
    "# optimized to reduce overfitting; good accuracy on small feature subsets; \n",
    "# terrible accuracy on full dataset; doesn't reduce train/test time\n",
    "# full hyper-parameter tuning needs to be carried out\n",
    "              DecisionTreeClassifier(ccp_alpha=0.1, random_state=20200408), \n",
    "              RandomForestClassifier(n_estimators=10, ccp_alpha=0.1, random_state=20200408),\n",
    "              DummyClassifier(strategy='most_frequent')\n",
    "             ]\n",
    "mlblclfopt = dict(zip(mlblclfnms, mlblclfobj))\n",
    "mlblnullcs = []\n",
    "#mlblnullcs = [('ohe', '1knn'),   # Manhattan works better for OrdinalEncoder \n",
    "#              ('ord', '2knn'),   # Euclidean works better for OneHotEncoder\n",
    "#              ('ord', 'dumbase') # DummyClassifier doesn't care for inp. reprs\n",
    "#             ]\n",
    "mlblclfpps = {(clf, enc, dim):\n",
    "               skpipe.make_pipeline(preprocessors[enc], middlelayer[dim],\n",
    "                                    mlblclfopt[clf])\n",
    "              for clf in mlblclfopt\n",
    "              for enc in preprocessors\n",
    "              for dim in middlelayer\n",
    "              if (enc, clf) not in mlblnullcs\n",
    "             }\n",
    "mlblclfpps = dict(sorted(mlblclfpps.items()))\n",
    "\n",
    "%time smlblclfacc = trainFullClassifiersCV(mlblclfpps, rawX, subYmlbl)\n",
    "if not BATCH:\n",
    "  smlblclfacc_ = smlblclfacc.dropna()\n",
    "  pd_displayHTML(smlblclfacc_.style.hide_index())\n",
    "  plot_accuracies(smlblclfacc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svPQiHs1tukV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "paramsknn = {'kneighborsclassifier__n_neighbors':range(3, 11), \n",
    "             'kneighborsclassifier__weights':('uniform', 'distance'),\n",
    "            #'kneighborsclassifier__algorithm':('auto', 'ball_tree', 'kd_tree', 'brute'),\n",
    "             'kneighborsclassifier__p':(1, 2), \n",
    "            }\n",
    "paramsmlp = {#'mlpclassifier__hidden_layer_sizes':[(size,) for size in range(50, 60, 20)], \n",
    "            #'mlpclassifier__activation:('relu', 'logistic', 'tanh'), \n",
    "            #'mlpclassifier__solver:('adam', 'lbfgs'), \n",
    "             'mlpclassifier__alpha':10.0**np.arange(-7, 4), \n",
    "            #'mlpclassifier__max_iter':range(200, 1001, 100),\n",
    "            #'mlpclassifier__early_stopping':(True, False),\n",
    "            }  \n",
    "paramsdt  = {'decisiontreeclassifier__criterion':('gini', 'entropy'), \n",
    "             'decisiontreeclassifier__max_features':('auto', 'sqrt', 'log2', None), \n",
    "             'decisiontreeclassifier__class_weight':(None, 'balanced'),\n",
    "             'decisiontreeclassifier__ccp_alpha':[i/10 for i in range(0, 11)], \n",
    "            #'decisiontreeclassifier__max_depth':, \n",
    "            #'decisiontreeclassifier__max_leaf_nodes':, \n",
    "            #'decisiontreeclassifier__min_impurity_decrease',\n",
    "            #'decisiontreeclassifier__min_impurity_split', \n",
    "            #'decisiontreeclassifier__min_samples_leaf', \n",
    "            #'decisiontreeclassifier__min_samples_split', \n",
    "            #'decisiontreeclassifier__min_weight_fraction_leaf', \n",
    "            #'decisiontreeclassifier__presort', \n",
    "            #'decisiontreeclassifier__random_state', \n",
    "            #'decisiontreeclassifier__splitter'\n",
    "            }\n",
    "paramsrf  = {'randomforestclassifier__n_estimators':range(10, 201, 10),\n",
    "            #'randomforestclassifier__criterion':('gini', 'entropy'),\n",
    "            #'randomforestclassifier__max_features':('auto', 'sqrt', 'log2', None),\n",
    "            #'randomforestclassifier__bootstrap':(True, False),\n",
    "            #'randomforestclassifier__class_weight':(None, 'balanced'), \n",
    "            #'randomforestclassifier__ccp_alpha':[i/10 for i in range(0, 11)],\n",
    "            #'randomforestclassifier__max_depth', \n",
    "            #'randomforestclassifier__min_samples_split', \n",
    "            #'randomforestclassifier__min_samples_leaf', \n",
    "            #'randomforestclassifier__min_weight_fraction_leaf',\n",
    "            #'randomforestclassifier__max_leaf_nodes', \n",
    "            #'randomforestclassifier__min_impurity_decrease', \n",
    "            #'randomforestclassifier__min_impurity_split', \n",
    "            #'randomforestclassifier__max_samples', \n",
    "            }\n",
    "paramsdummy = {'dummyclassifier__strategy':('stratified', 'most_frequent', 'prior', 'uniform'),\n",
    "              #'dummyclassifier__constant':(1), \n",
    "              #'dummyclassifier__random_state', \n",
    "              }\n",
    "\n",
    "#print(mlblclfpls['ohe/dumbase'].get_params().keys())\n",
    "grdmlblclf = GridSearchCV(mlblclfpps[('dumbase', 'ohe', 'dim0')], param_grid=paramsdummy,\n",
    "                         cv=cvsplits, n_jobs=2).fit(rawX, subYmlbl)\n",
    "print(grdmlblclf.best_params_)\n",
    "grdmlblres = pd.DataFrame(grdmlblclf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7n2fFNpv_pE"
   },
   "outputs": [],
   "source": [
    "%time mlblclfacc = trainFullClassifiersCV(mlblclfpps, rawX, Ymlbl)\n",
    "if not BATCH:\n",
    "  mlblclfacc_ = mlblclfacc.dropna()\n",
    "  pd_displayHTML(mlblclfacc_.style.hide_index())\n",
    "  plot_accuracies(mlblclfacc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YdaXBzY6saXL"
   },
   "outputs": [],
   "source": [
    "expreport = []\n",
    "try:\n",
    "  if not BATCH:\n",
    "    raise UserWarning(\"Following code in this cell can only be run with BATCH=True from cmd-line\")\n",
    "  CVFOLDS, REPEAT = 2, 5 \n",
    "  samplec, featcsc = alablFull.shape[0], alablFull.shape[1]\n",
    "  subsid = list(range(samplec))  \n",
    "  #params = [(k, 10) for k in range(5, fnccount, 10)]\n",
    "  params = [(CVFOLDS, REPEAT, k) for k in range(5, fnccount, 10)]\n",
    "  ATTEMPTS = 10\n",
    "  #params = list(params)[:1]\n",
    "\n",
    "  # this is to make sure that this block can be run in standalone mode\n",
    "  featsFull_ = featsFull.iloc[subsid,:]\n",
    "  clablFull_ = clablFull.iloc[subsid]\n",
    "  alablFull_ = alablFull.iloc[subsid,:]\n",
    "\n",
    "  X   = featsFull_.copy(deep=False)\n",
    "  ccs = X.select_dtypes(['category']).columns \n",
    "  X[ccs] = X[ccs].apply(lambda x: x.cat.codes)\n",
    "  X  = X.to_numpy()\n",
    "  \n",
    "  Y = clablFull_.copy(deep=False).cat.codes\n",
    "  Y  = Y.to_numpy()\n",
    "\n",
    "  Y_  = alablFull_.copy(deep=False)\n",
    "  ccs = Y_.select_dtypes(['category']).columns\n",
    "  Y_[ccs] = Y_[ccs].apply(lambda x: x.cat.codes)\n",
    "  Y_ = Y_.to_numpy()\n",
    "  \n",
    "  Ymlbl = np.zeros((X.shape[0], alablTabl.shape[0]))\n",
    "  filidx = np.array([ (irow, alablTabl.loc['{0}={1}'.format(fcn, lbl), 'id'])\n",
    "                    for irow, row in enumerate(alablFull_.to_dict(orient='records'))\n",
    "                    for fcn, lbl in row.items() if not pd.isna(lbl)\n",
    "                   ])\n",
    "  Ymlbl[[filidx[:,0], filidx[:,1]]] = 1\n",
    "\n",
    "  for expparam in params:\n",
    "    cvsplits = list(RepeatedKFold(n_splits=expparam[0], \n",
    "                                  n_repeats=expparam[1], random_state=20200408\n",
    "                                 ).split(X, Y))\n",
    "    expr1 = trainFullClassifiersCV(classifiers, X, Y)\n",
    "    # run experiment using X,Y\n",
    "    expreport.extend(dict([('ExpName', 'fulllbl-dense'), \n",
    "                           ('CVF', expparam[0]),\n",
    "                           ('REPEAT', expparam[1]), \n",
    "                           ('K', expparam[2]),\n",
    "                           ('Params', 'default')\n",
    "                          ] + \\\n",
    "                          list(row.items()))\n",
    "                     for row in expr1.to_dict(orient='records')\n",
    "                    ) \n",
    "    expr2 = trainFullClassifiersCV(mlblclasfrs, X, Ymlbl)\n",
    "    expreport.extend(dict([('ExpName', 'fulllbl-sparse'), \n",
    "                           ('CVF', expparam[0]),\n",
    "                           ('REPEAT', expparam[1]), \n",
    "                           ('K', expparam[2]),\n",
    "                           ('Params', 'default')\n",
    "                          ] + \\\n",
    "                          list(row.items()))\n",
    "                     for row in expr2.to_dict(orient='records')\n",
    "                    ) \n",
    "\n",
    "    choices = ncombr(fnccount, expparam[2])\n",
    "    for trial in range(1, ATTEMPTS+1):  #int(FRACP*choices)\n",
    "      subfci = list(sorted(random.sample(range(fnccount), expparam[2])))\n",
    "      subfcs = list(alablFull.columns[i] for i in subfci)\n",
    "      \n",
    "      alablSub_  = alablFull.iloc[subsid,subfci]\n",
    "\n",
    "      clablSub = ['|'.join('{0}={1}'.format(k,v)\n",
    "                           for k,v in row.items() if not pd.isna(v))\n",
    "                  for row in alablSub_.to_dict(orient='records')\n",
    "                 ]\n",
    "      clablSub = pd.Series(clablSub, name=header[-1])\n",
    "      subY = clablSub.astype('category').cat.codes\n",
    "      subY = subY.to_numpy()\n",
    "\n",
    "      subY_ = alablSub_.copy(deep=False)\n",
    "      ccs = subY_.select_dtypes(['category']).columns\n",
    "      subY_[ccs] = subY_[ccs].apply(lambda x: x.cat.codes)\n",
    "      subY_ = subY_.to_numpy()\n",
    "\n",
    "      subYmlbl = np.zeros((Y.shape[0], alablTabl.shape[0]))\n",
    "      filidx = np.array([ (irow, alablTabl.loc['{0}={1}'.format(fcn, lbl), 'id'])\n",
    "                         for irow, row in enumerate(alablSub_.to_dict(orient='records'))\n",
    "                         for fcn, lbl in row.items() if not pd.isna(lbl)\n",
    "                       ])\n",
    "      subYmlbl[[filidx[:,0], filidx[:,1]]] = 1\n",
    "      \n",
    "      expr1 = trainFullClassifiersCV(classifiers, X, subY)\n",
    "      expreport.extend(dict([('ExpName', 'sublbl-dense'), ('CVF', expparam[0]),\n",
    "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
    "                             ('TRIAL', 'T{0}'.format(trial)), \n",
    "                             ('Params', 'default')\n",
    "                            ] + \\\n",
    "                            list(row.items()))\n",
    "                       for row in expr1.to_dict(orient='records')) \n",
    "      \n",
    "      expr2 = trainFullClassifiersCV(mlblclasfrs, X, subYmlbl)\n",
    "      expreport.extend(dict([('ExpName', 'sublbl-sparse'), ('CVF', expparam[0]),\n",
    "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
    "                             ('TRIAL', 'T{0}'.format(trial)), \n",
    "                             ('Params', 'default')\n",
    "                            ] + \\\n",
    "                            list(row.items()))\n",
    "                       for row in expr2.to_dict(orient='records')) \n",
    "      \n",
    "      expr3, clfs = trainIndClassifiersCV(classifiers, X, subY_, return_clfinsts=True)\n",
    "      expreport.extend(dict([('ExpName', 'sublbl-dense-ind'), ('CVF', expparam[0]),\n",
    "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
    "                             ('TRIAL', 'T{0}'.format(trial)),\n",
    "                             ('Params', 'default')\n",
    "                            ] + \\\n",
    "                            list(row.items()))\n",
    "                       for row in expr3.to_dict(orient='records')) \n",
    "      \n",
    "      expr4 = jntTestIndClassifiersCV(classifiers, X, subY_, clfs)\n",
    "      expreport.extend(dict([('ExpName', 'sublbl-dense-jnt'), ('CVF', expparam[0]),\n",
    "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
    "                             ('TRIAL', 'T{0}'.format(trial)),\n",
    "                             ('Params', 'default')\n",
    "                            ] + \\\n",
    "                            list(row.items()))\n",
    "                       for row in expr4.to_dict(orient='records')) \n",
    "  \n",
    "  pd.DataFrame(expreport).to_html('sigtyp-st2020-part1-batchexps-results.html', index=False)\n",
    "  pd.DataFrame(expreport).to_json('sigtyp-st2020-part1-batchexps-results.json')\n",
    "except KeyboardInterrupt:\n",
    "  pd.DataFrame(expreport).to_html('sigtyp-st2020-part1-batchexps-results-aborted.html', index=False)\n",
    "  pd.DataFrame(expreport).to_json('sigtyp-st2020-part1-batchexps-results-aborted.json')\n",
    "except UserWarning as err:\n",
    "  print(err)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "sigtyp-st2020-part1.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
