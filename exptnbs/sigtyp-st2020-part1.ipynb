{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sigtyp-st2020-part1.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkolachi/geodist2typfeat/blob/master/exptnbs/sigtyp-st2020-part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nuSUXKKMhvlL",
        "colab": {}
      },
      "source": [
        "%autosave 60\n",
        "%matplotlib inline\n",
        "%pylab\n",
        "\n",
        "fpurl   = 'https://raw.githubusercontent.com/sigtyp/ST2020/master/data/train.csv'\n",
        "# the header from the csv is not properly tab-seperated. hence hard-coding\n",
        "header  = ['wals_code', 'name', \n",
        "           'latitude', 'longitude', \n",
        "           'genus', 'family', 'countrycodes', \n",
        "           'features'\n",
        "          ]\n",
        "\n",
        "CVFOLDS = 2   # default: 2 folds\n",
        "REPEAT = 2\n",
        "\n",
        "N = -1        # default: use all samples \n",
        "K = 10        # default: use only 5 feature classes\n",
        "\n",
        "# turn this on iff running from command-line to test performance across \n",
        "# different values for (CVFOLDS, K, REPEAT) \n",
        "BATCH = False  \n",
        "\n",
        "import itertools as it\n",
        "from collections import Counter, defaultdict\n",
        "from operator    import itemgetter\n",
        "from IPython.display import display as pd_displayHTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OXxOiiVAmiSW",
        "colab": {}
      },
      "source": [
        "#%pip install -q pycodestyle_magic flake8\n",
        "#%load_ext pycodestyle_magic"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2_kKY1tspATV",
        "colab": {}
      },
      "source": [
        "#%flake8_on -m 119 --ignore=E111"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ka9wrCHqzQ7J"
      },
      "source": [
        "I hoped the provided train/test dataset is CSV compliant so that loading the dataset is as simple as using *pandas.read_csv*. It turned out not to be the case. The problem is with the header in the provided csv file, which makes inferring the columns using *header=auto* impossible. This is easily handled by hard-coding the column names in the header and skipping the first row when using *pandas.read_csv*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E1o-kKUMp7HD",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "%pip install -q --user pandas==1.0.3\n",
        "import pandas as pd\n",
        "df = pd.read_csv(fpurl, sep='\\t', header=None, names=header,\n",
        "                 #index_col=0,\n",
        "                 error_bad_lines=True, skiprows=[0])\n",
        "\"\"\"\n",
        "# since this pynb will never be run on the held-out test set\n",
        "if CVFOLDS <= 1:\n",
        "  trnS, tstS = 0, 0   # dummy values for sizes of train and test partitions\n",
        "else:\n",
        "  tstdf = pd.read_csv(tstfpurl, sep='\\t', header=None, names=header,\n",
        "                      error_bad_lines=True, skiprows=[0])\n",
        "  trnS, tstS = df.shape[0], tstdf.shape[0]\n",
        "  df.append(tstdf)\n",
        "\"\"\"\n",
        "missingVal, missingLbl = '*-missing-*', '*-unknown-*'\n",
        "featsFull = df.iloc[:, 0:-1]\n",
        "clablFull = df.iloc[:, -1]\n",
        "alablInst = Counter(albl for inst in clablFull for albl in inst.split('|'))\n",
        "alablTabl = pd.DataFrame([{'name': n, 'id': i, 'freq': f}\n",
        "                          for i,(n,f) in enumerate(alablInst.most_common(), start=1)\n",
        "                         ]).set_index('name')\n",
        "alablFull = pd.DataFrame([dict(albl.split('=', 1) for albl in inst.split('|'))\n",
        "                          for inst in clablFull\n",
        "                         ]).fillna(missingLbl) # fill missing values (no NaN) \n",
        "for incol in ['wals_code', 'name', 'genus', 'family', 'countrycodes']:\n",
        "  featsFull[incol] = featsFull[incol].astype('category')\n",
        "clablFull = clablFull.astype('category')\n",
        "alablFull = alablFull.astype('category')\n",
        "\n",
        "print(featsFull.shape, clablFull.shape, alablFull.shape, alablTabl.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef-hBcBz-w4r",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the dataset\n",
        "\n",
        "Let us visualize a few statistics about the dataset \n",
        "1.   Histogram of the complex labels in the dataset\n",
        "2.   Histogram of the atomic labels for each feature class in the dataset\n",
        "3.   Scatterplots of genus vs labels, family vs labels and countrycodes vs labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y1p-kfcpy0Yc",
        "colab": {}
      },
      "source": [
        "%pip install -q --user seaborn==0.10.0\n",
        "import seaborn as sns\n",
        "def plot_datastats(features, clabels, alabels):\n",
        "  return\n",
        "\n",
        "if not BATCH:\n",
        "  plot_datastats(featsFull, clablFull, alablFull)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAFMfwEbPZg3",
        "colab_type": "text"
      },
      "source": [
        "### Adding features\n",
        "\n",
        "##### Ideas:\n",
        "+    Convert latitude and longitude to UTM which can be encoded as a discrete feature\n",
        "    -  [wiki description](https://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system) \n",
        "    -  [source code](https://github.com/Turbo87/utm) to use \n",
        "    - [kaggle question](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/discussion/62711) on similar topic \n",
        "\n",
        "The dataset is loaded into a DataFrame and seperated into two parts: input features and output labels. \n",
        "\n",
        "We know a few things about the input features like what are categorical features and what are numerical features. So, we encode the different columns in the feats DataFrame accordingly. *Hopefully this matters* when training different classifiers (especially thinking of decision trees). \n",
        "\n",
        "At this point, I'm not looking at best encoding scheme for the labels which are composite labels themselves (more on this later). The training dataset provided has 1109 unique labels for the dataset of 1125 languages. This indicates that there is *an optimal representation* for the label set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oheDgj5G4wL7",
        "colab_type": "text"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pxmCRll3lMr",
        "colab": {}
      },
      "source": [
        "# sub-select data frame to speed-up experiments while debugging\n",
        "import random\n",
        "# because we want sampling without replacement when we work with selected\n",
        "# features classes to test, using random makes statistics across runs\n",
        "# incomparable -- so use a uniform distribution to select feature classes for\n",
        "# comparison across different experiments.\n",
        "# to get robust estimates while testing, use random selection\n",
        "if N < 2 or N > featsFull.shape[0]:\n",
        "  subsid = list(range(featsFull.shape[0]))\n",
        "else:\n",
        "  subsid = list(range(0, featsFull.shape[0], featsFull.shape[0]//N))[:N]\n",
        "\n",
        "if K < 0 or K > alablFull.shape[1]:\n",
        "  subfci = list(range(alablFull.shape[1]))\n",
        "else:\n",
        "  subfci = list(sorted(random.sample(range(alablFull.shape[1]), K)))\n",
        "  #subfci = list(range(0, alablFull.shape[1], alablFull.shape[1]//K))[:K])\n",
        "subfcs = list(alablFull.columns[i] for i in subfci)\n",
        "\n",
        "#relevfeats = [i for i,f in enumerate(header)][2:-1]\n",
        "featsFull_ = featsFull.iloc[subsid, :]\n",
        "clablFull_ = clablFull.iloc[subsid]\n",
        "alablSub_  = alablFull.iloc[subsid, subfci]\n",
        "alablFull_ = alablFull.iloc[subsid, :]\n",
        "\n",
        "print(featsFull_.shape, clablFull_.shape, alablFull_.shape, alablSub_.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I28aYTkvm6TI"
      },
      "source": [
        "### Input representations and Output encodings\n",
        "\n",
        "\n",
        "Let us try a few classifiers using *scikit-learn* at this point. \n",
        "\n",
        "For what it is worth, the accuracies can be worse than a coin flip, considering the sparse label set.\n",
        "\n",
        "\n",
        " features of languages spoken in close proximity and belonging to the same family should be highly informative in predicting the typographical features for a new language. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PJZ0u8vkZ3XB",
        "colab": {}
      },
      "source": [
        "deprecated = \"\"\"\n",
        "# it is essential to make deep copies of the frame when building numpy\n",
        "# matrices used for classification experiments.\n",
        "# if not, changes to the matrix representations e.g. encoding categorial\n",
        "# variables as ordinals or sparse-matrices are reflected in the original frame\n",
        "# which results in errors when trying to re-use the frames for other experiments\n",
        "# e.g. lookup in the atomic-label table built above results in errors because\n",
        "# the lookup tries to find fnc=lbl-idx where idx is the category code\n",
        "X   = featsFull_.copy(deep=False)\n",
        "ccs = X.select_dtypes(['category']).columns\n",
        "X[ccs] = X[ccs].apply(lambda x: x.cat.codes)\n",
        "\n",
        "Y = clablFull_.copy(deep=False).cat.codes\n",
        "\n",
        "Y_  = alablFull_.copy(deep=False)\n",
        "ccs = Y_.select_dtypes(['category']).columns\n",
        "Y_[ccs] = Y_[ccs].apply(lambda x: x.cat.codes)\n",
        "\n",
        "subY_ = alablSub_.copy(deep=False)\n",
        "ccs = subY_.select_dtypes(['category']).columns\n",
        "subY_[ccs] = subY_[ccs].apply(lambda x: x.cat.codes)\n",
        "\n",
        "X  = X.to_numpy()\n",
        "Y  = Y.to_numpy()\n",
        "Y_ = Y_.to_numpy()\n",
        "subY_ = subY_.to_numpy()\n",
        "\n",
        "print(X.shape, Y.shape, Y_.shape, subY_.shape)\n",
        "\"\"\";"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kadqK9JkhKKw",
        "colab": {}
      },
      "source": [
        " %pip install -q --user scikit-learn==0.22.2.post1\n",
        "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder \n",
        "\n",
        "lblenc = LabelEncoder().fit(clablFull_)\n",
        "Ynms   = lblenc.classes_\n",
        "Y      = lblenc.transform(clablFull_)\n",
        "\n",
        "lblenc = OrdinalEncoder().fit(alablFull_)\n",
        "aYnms  = lblenc.categories_\n",
        "Y_     = lblenc.transform(alablFull_)\n",
        "\n",
        "mlablFull = [[alablTabl.loc['{0}={1}'.format(fcn, lbl),'id']\n",
        "              for fcn,lbl in row.items() if lbl != missingLbl]\n",
        "             for row in alablFull_.to_dict(orient='records')\n",
        "            ]\n",
        "Ymlbl = MultiLabelBinarizer().fit_transform(mlablFull)\n",
        "\n",
        "rawX = featsFull_.copy(deep=False)\n",
        "\n",
        "print(Y.shape, Y_.shape, Ymlbl.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JBPJvqvDNJcj",
        "colab": {}
      },
      "source": [
        "if len(subfci) < alablFull_.shape[1]:\n",
        "  clablSub = ['|'.join('{0}={1}'.format(fcn, lbl) for fcn, lbl in row.items()\n",
        "                       if lbl != missingLbl)\n",
        "              for row in alablSub_.to_dict(orient='records')\n",
        "             ]\n",
        "  lblenc   = LabelEncoder().fit(clablSub)\n",
        "  subYnms  = lblenc.classes_\n",
        "  subY     = lblenc.transform(clablSub)\n",
        "\n",
        "  lblenc   = OrdinalEncoder().fit(alablSub_)\n",
        "  subaYnms = lblenc.categories_\n",
        "  subY_    = lblenc.transform(alablSub_)\n",
        "  \n",
        "  mlablSub = [[alablTabl.loc['{0}={1}'.format(fcn, lbl),'id']\n",
        "               for fcn,lbl in row.items() if lbl != missingLbl]\n",
        "              for row in alablSub_.to_dict(orient='records')\n",
        "             ]\n",
        "  subYmlbl = MultiLabelBinarizer().fit_transform(mlablSub)\n",
        "else:\n",
        "  subY, subY_, subYmlbl = Y, Y_, Ymlbl\n",
        "\n",
        "print(subY.shape, subY_.shape, subYmlbl.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NzMTKIUHWNko",
        "colab": {}
      },
      "source": [
        "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
        "# https://scikit-learn.org/stable/modules/cross_validation.html\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
        "\n",
        "from sklearn import pipeline as skpipe\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder \n",
        "\n",
        "numfxr = SimpleImputer(strategy='median')\n",
        "strfxr = SimpleImputer(strategy='constant', fill_value=missingVal)\n",
        "strfx_ = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "stdtrn = StandardScaler()\n",
        "ohetrn = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "ordtrn = OrdinalEncoder(categories='auto')\n",
        "\n",
        "# numerical and categorial features in the given data\n",
        "numfeats = ['latitude', 'longitude']\n",
        "catfeats = ['wals_code', 'name', 'genus', 'family', 'countrycodes'] \n",
        "\n",
        "# for numerical features, fill unknown feature values with median value and \n",
        "# scale the column with mean and variance\n",
        "numtrans = skpipe.Pipeline(steps=[('imputer', numfxr), ('transform',  stdtrn)])\n",
        "\n",
        "# for categorial features, try both one-hot encoding and ordinal encoding\n",
        "ohcattrans = skpipe.Pipeline(steps=[('imputer', strfxr), ('transform', ohetrn)])\n",
        "cohtrans = ColumnTransformer(transformers=[('num', numtrans, numfeats), \n",
        "                                           ('cat', ohcattrans, catfeats)])\n",
        "\n",
        "oecattrans = skpipe.Pipeline(steps=[('imputer', strfxr), ('transform', ordtrn)])\n",
        "coetrans = ColumnTransformer(transformers=[('num', numtrans, numfeats), \n",
        "                                           ('cat', oecattrans, catfeats)])\n",
        "# There are known issues in sklearn when integrating OrdinalFeatures into the \n",
        "# pipeline that cause problems with the standard/usual way of incorporating a \n",
        "# transformer to extract features. \n",
        "# Below is a 'hacky' version that gets around this and simulates the use of \n",
        "# dense numerical features for this classification task -- note that, this is\n",
        "# still a hack\n",
        "uniqcats = [rawX.loc[:,catn].unique() for catn in catfeats]\n",
        "oetrans_ = skpipe.Pipeline(steps=[('imputer', strfx_),\n",
        "                                  ('transform', OrdinalEncoder(categories=uniqcats))])\n",
        "altoetrn = ColumnTransformer(transformers=[('num', numtrans, numfeats), \n",
        "                                           ('cat', oetrans_, catfeats)])\n",
        "\n",
        "preprocessors = {'ohe': cohtrans,\n",
        "                #'ord': coetrans,\n",
        "                 'ord': altoetrn,\n",
        "                }\n",
        "\n",
        "# Dimensionality reduction techniques \n",
        "opca = PCA(svd_solver='arpack', random_state=20200408)\n",
        "osvd = TruncatedSVD(algorithm='arpack', random_state=20200408)\n",
        "npca = skpipe.Pipeline(steps=[('dimred', opca), ('dimscale', stdtrn)])\n",
        "nsvd = skpipe.Pipeline(steps=[('dimred', osvd), ('dimscale', stdtrn)])\n",
        "\n",
        "featreprtrans = {'dim0': 'passthrough', \n",
        "                #'pca':  opca, \n",
        "                #'svd':  osvd, \n",
        "                 'npca': npca, \n",
        "                 'nsvd': nsvd\n",
        "                }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF7eCmGwBV9z",
        "colab_type": "text"
      },
      "source": [
        "### Multi-class classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8GbLxqzoGwwL",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# all these classifiers support multi-class classification in sklearn\n",
        "mclsclfnms = ['knn', 'lsvm', 'svc', 'mlp', 'dt', \n",
        "              'rf', 'adb', \n",
        "              'nb', 'ridge', 'dumbase',\n",
        "             #'gp', 'qda', \n",
        "             ]\n",
        "mclsclfobj = [KNeighborsClassifier(p=1), # works well for all inputs \n",
        "              LinearSVC(penalty='l1', dual=False, C=0.01, random_state=20200408),\n",
        "              SVC(gamma=2, random_state=20200408),\n",
        "              MLPClassifier(activation='logistic', solver='adam', alpha=10, \n",
        "                           early_stopping=False, max_iter=1000,\n",
        "                            random_state=20200408),\n",
        "              DecisionTreeClassifier(max_features='auto', ccp_alpha=0.1, \n",
        "                                    random_state=20200408),\n",
        "              RandomForestClassifier(n_estimators=10, criterion='entropy', \n",
        "                                    ccp_alpha=0.1, random_state=20200408),\n",
        "              AdaBoostClassifier(random_state=20200408),\n",
        "              GaussianNB(),\n",
        "              RidgeClassifier(random_state=20200408),\n",
        "              DummyClassifier(strategy=\"most_frequent\"),\n",
        "             #GaussianProcessClassifier(),\n",
        "             #QuadraticDiscriminantAnalysis(),\n",
        "             ]\n",
        "mclsclfopt = dict(zip(mclsclfnms, mclsclfobj))\n",
        "\n",
        "# setup pipelines for all combination of preprocessors and classifiers\n",
        "pipelines  = {(clf, enc, dim):\n",
        "               skpipe.make_pipeline(preprocessors[enc], featreprtrans[dim],\n",
        "                                    mclsclfopt[clf])\n",
        "              for (clf, enc, dim) in it.product(mclsclfopt, preprocessors, featreprtrans)\n",
        "             }\n",
        "pipelines  = dict(sorted(pipelines.items()))\n",
        "\n",
        "from sklearn.model_selection import KFold, RepeatedKFold\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
        "REPEAT  = REPEAT  if REPEAT  >  1 else 1  # sanity-check\n",
        "CVFOLDS = CVFOLDS if CVFOLDS >= 2 else 2  # sanity-check\n",
        "# scikit-learn documentation recommends using StratifiedKFold for classification\n",
        "# problems to preserve class balance across folds. however, in this case, \n",
        "# we use KFold and RepeatedKFold because \n",
        "#  number of items in a class <= CVFOLDS (works only with 2 folds for entire dataset)\n",
        "#  there is not much balance to preserve w.r.t. complex labels\n",
        "cvsplits = list(RepeatedKFold(n_splits=CVFOLDS, \n",
        "                              n_repeats=REPEAT, random_state=20200408\n",
        "                             ).split(rawX, Y))\n",
        "\n",
        "statnames = ['classifier', \n",
        "             'avg.acc/tst', 'avg.acc/trn', 'std.acc/tst', 'std.acc/trn', \n",
        "             'avg.time/prd', 'avg.time/trn'] \n",
        "statcodes = ['clfn', 'mtsts', 'mtrns', 'vtsts', 'vtrns', 'predt', 'trint']\n",
        "\n",
        "# https://machinelearningmastery.com/how-to-fix-futurewarning-messages-in-scikit-learn/\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=DeprecationWarning)\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.exceptions import ConvergenceWarning, FitFailedWarning\n",
        "simplefilter(action='always', category=ConvergenceWarning)\n",
        "simplefilter(action='always', category=FitFailedWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL4wt-hYIrs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dmbcmbs = [_ for _ in pipelines if _[0] == 'dumbase'][1:]\n",
        "for cmb in dmbcmbs: del pipelines[cmb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D-qdcL_F4cmi",
        "colab": {}
      },
      "source": [
        "def plot_accuracies(accuracies, savefile=None):\n",
        "  hastime = 'avg.time/prd' in accuracies.columns and 'avg.time/trn' in accuracies.columns\n",
        "\n",
        "  accuracies_ = accuracies.iloc[::-1]\n",
        "  nrs, ncs = 1, 4\n",
        "  o, r, m, n = tuple(range(4))\n",
        "  lefts = [0 for _ in range(accuracies.shape[0])]\n",
        "  fig, axs = plt.subplots(nrs, ncs, sharey=True, sharex='col', \n",
        "                         squeeze=False, \n",
        "                         figsize=(6.4, 0.25*accuracies.shape[0]))\n",
        "  axs[o//ncs][o%ncs].barh(accuracies_['classifier'], accuracies_['avg.acc/tst'], \n",
        "                          left=lefts, \n",
        "                          xerr=accuracies_['std.acc/tst'])\n",
        "  axs[o//ncs][o%ncs].text(.5, .98, 'held-out', horizontalalignment='center', \n",
        "                          transform=axs[o//ncs][o%ncs].transAxes)\n",
        "  axs[r//ncs][r%ncs].barh(accuracies_['classifier'], accuracies_['avg.acc/trn'], \n",
        "                          left=lefts, \n",
        "                          xerr=accuracies_['std.acc/trn'])\n",
        "  axs[r//ncs][r%ncs].text(.5, .98, 'training', horizontalalignment='center', \n",
        "                          transform=axs[r//ncs][r%ncs].transAxes)\n",
        "  if hastime:\n",
        "    axs[m//ncs][m%ncs].barh(accuracies_['classifier'], accuracies_['avg.time/prd'], \n",
        "                            left=lefts)\n",
        "    axs[m//ncs][m%ncs].text(.5, .98, 'prediction', horizontalalignment='center', \n",
        "                            transform=axs[m//ncs][m%ncs].transAxes)\n",
        "    axs[n//ncs][n%ncs].barh(accuracies_['classifier'], accuracies_['avg.time/trn'], \n",
        "                            left=lefts)\n",
        "    axs[n//ncs][n%ncs].text(.5, .98, 'training', horizontalalignment='center', \n",
        "                            transform=axs[n//ncs][n%ncs].transAxes)\n",
        "  axs[o//ncs][o%ncs].set_xlabel('Accuracy')\n",
        "  axs[r//ncs][r%ncs].set_xlabel('Accuracy')\n",
        "  axs[m//ncs][m%ncs].set_xlabel('Time(s)')\n",
        "  axs[n//ncs][n%ncs].set_xlabel('Time(s)')\n",
        "  # minimize space between subplots\n",
        "  plt.subplots_adjust(hspace=0)\n",
        "\n",
        "  if not savefile:\n",
        "    plt.show()\n",
        "  else:\n",
        "    plt.savefig(savefile)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qWvm2I-2oPwg",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection as skms\n",
        "\n",
        "codify_classifier  = lambda clf: '/'.join(clf) if isinstance(clf, (list, tuple, set)) else str(clf) \n",
        "select_nclfcombos = lambda df, n: df.sort_values(by=['avg.acc/tst', \n",
        "                                                     'avg.acc/trn'], \n",
        "                                  ascending=False).groupby(lambda rid: \n",
        "  df.loc[rid, 'classifier'].split('/')[0]).head(n).sort_values(by=['classifier'])\n",
        "select_clfcombos  = lambda df: select_nclfcombos(df, 1)\n",
        "\n",
        "def trainFullClassifiersCV(classifiers, X, Y):\n",
        "  clfaccs = []\n",
        "  for iclf, nclf in enumerate(classifiers):\n",
        "    clfsce = skms.cross_validate(classifiers[nclf], X, Y, cv=cvsplits, \n",
        "                                 return_train_score=True\n",
        "                                )\n",
        "    clfnfo = [codify_classifier(nclf), \n",
        "              100*clfsce['test_score'].mean(), 100*clfsce['train_score'].mean(),\n",
        "              100*clfsce['test_score'].std(),  100*clfsce['train_score'].std(),\n",
        "              clfsce['score_time'].sum(), clfsce['fit_time'].sum()\n",
        "             ]\n",
        "    clfaccs.append(dict(zip(statnames, clfnfo)))\n",
        "  return pd.DataFrame(clfaccs)\n",
        "\n",
        "%time clfaccs = trainFullClassifiersCV(pipelines, rawX, Y)\n",
        "if not BATCH:\n",
        "  clfaccs_ = clfaccs.dropna()\n",
        "  plot_accuracies(clfaccs_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fvX9mw94saLx",
        "colab": {}
      },
      "source": [
        "%time sclfaccs = trainFullClassifiersCV(pipelines, rawX, subY)\n",
        "if not BATCH:\n",
        "  sclfaccs_ = sclfaccs.dropna()\n",
        "  #print(select_clfcombos(sclfaccs_).round(3).to_markdown(showindex=False))\n",
        "  pd_displayHTML(select_clfcombos(sclfaccs_).style.hide_index())\n",
        "  plot_accuracies(sclfaccs_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZV_8hiBB5v",
        "colab_type": "text"
      },
      "source": [
        "### Multi-label Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3eFDaf8sy-b",
        "colab": {}
      },
      "source": [
        "mlblclfnms = ['1knn', '2knn', 'mlp', 'dt', 'rf', 'dumbase']\n",
        "mlblclfobj = [KNeighborsClassifier(p=1), # Manhattan distance\n",
        "              KNeighborsClassifier(p=2), # Euclidean distance\n",
        "              MLPClassifier(activation='logistic', solver='lbfgs', alpha=10, \n",
        "                           early_stopping=True, max_iter=1000,\n",
        "                           random_state=20200408),\n",
        "# optimized to reduce overfitting; good accuracy on small feature subsets; \n",
        "# terrible accuracy on full dataset; doesn't reduce train/test time\n",
        "# full hyper-parameter tuning needs to be carried out\n",
        "              DecisionTreeClassifier(ccp_alpha=0.1, random_state=20200408), \n",
        "              RandomForestClassifier(n_estimators=10, ccp_alpha=0.1, \n",
        "                                    random_state=20200408),\n",
        "              DummyClassifier(strategy='most_frequent')\n",
        "             ]\n",
        "mlblclfopt = dict(zip(mlblclfnms, mlblclfobj))\n",
        "mlblclfpls = {(clf, enc, dim):\n",
        "               skpipe.make_pipeline(preprocessors[enc], featreprtrans[dim],\n",
        "                                    mlblclfopt[clf])\n",
        "              for (clf, enc, dim) in it.product(mlblclfopt, preprocessors, featreprtrans)\n",
        "             }\n",
        "mlblclfpls = dict(sorted(mlblclfpls.items()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQCTnpFILyXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dmbcmbs = [_ for _ in mlblclfpls if _[0] == 'dumbase'][1:]\n",
        "for cmb in dmbcmbs: del mlblclfpls[cmb]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S80L4VFGLyjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time smlblclfacc = trainFullClassifiersCV(mlblclfpls, rawX, subYmlbl)\n",
        "if not BATCH:\n",
        "  smlblclfacc_ = smlblclfacc.dropna()\n",
        "  pd_displayHTML(select_clfcombos(smlblclfacc_).style.hide_index())\n",
        "  plot_accuracies(smlblclfacc_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y7n2fFNpv_pE",
        "colab": {}
      },
      "source": [
        "%time mlblclfacc = trainFullClassifiersCV(mlblclfpls, rawX, Ymlbl)\n",
        "if not BATCH:\n",
        "  mlblclfacc_ = mlblclfacc.dropna()\n",
        "  pd_displayHTML(select_clfcombos(mlblclfacc_).style.hide_index())\n",
        "  plot_accuracies(mlblclfacc_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNcDu8sQEdE2",
        "colab_type": "text"
      },
      "source": [
        "#### Multi-label Multi-class Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t3IC4j3K0_SW",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection as skms\n",
        "\n",
        "def trainIndClassifiersCV(classifiers, X, matY, return_clfinsts=False):\n",
        "  lclfinst  = {}  # table to store classifiers for later use\n",
        "  lclfaccs = np.zeros((matY.shape[-1], len(classifiers), 6))\n",
        "  avgcaccs = []\n",
        "  for iclf, nclf in enumerate(classifiers):\n",
        "    try:\n",
        "      for indY in range(matY.shape[-1]):\n",
        "        clfsce = skms.cross_validate(classifiers[nclf], X, matY[:,indY], \n",
        "                                     cv=cvsplits, \n",
        "                                     return_train_score=True, \n",
        "                                     return_estimator=return_clfinsts\n",
        "                                    )\n",
        "        lclfaccs[indY][iclf] = [100*clfsce['test_score'].mean(), \n",
        "                                100*clfsce['train_score'].mean(), \n",
        "                                100*clfsce['test_score'].std(),  \n",
        "                                100*clfsce['train_score'].std(), \n",
        "                                clfsce['score_time'].sum(), \n",
        "                                clfsce['fit_time'].sum()\n",
        "                               ]\n",
        "        if return_clfinsts:\n",
        "          lclfinst[(nclf, indY)] = clfsce['estimator']\n",
        "    except FitFailedWarning:\n",
        "      # remove all instances of estimators for this classifier setup\n",
        "      for fclf in (_ for _ in lclfinst if _[0] == nclf):\n",
        "        del lclfinst[fclf] \n",
        "    clfnfo = [codify_classifier(nclf), \n",
        "              lclfaccs[:,iclf,0].mean(), lclfaccs[:,iclf,1].mean(),\n",
        "              lclfaccs[:,iclf,2].mean(), lclfaccs[:,iclf,3].mean(),\n",
        "              lclfaccs[:,iclf,4].sum(),  lclfaccs[:,iclf,5].sum()\n",
        "             ]\n",
        "    avgcaccs.append(dict(zip(statnames, clfnfo)))\n",
        "  if return_clfinsts:\n",
        "    return (pd.DataFrame(avgcaccs), lclfinst)\n",
        "  else:\n",
        "    return pd.DataFrame(avgcaccs)\n",
        "\n",
        "%time avgcaccs, lclclfs = trainIndClassifiersCV(pipelines, rawX, subY_, return_clfinsts=True)\n",
        "if not BATCH:\n",
        "  avgcaccs_ = avgcaccs.dropna()\n",
        "  pd_displayHTML(select_clfcombos(avgcaccs_).style.hide_index())\n",
        "  plot_accuracies(avgcaccs_)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XXQ4O5uXkZHZ",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection as skms\n",
        "from sklearn import metrics as skmt\n",
        "from sklearn.exceptions import NotFittedError\n",
        "\n",
        "def skmt_mlmc_accuracy_score(y_true, y_pred):\n",
        "  \"Classification accuracy for multi-label multi-class problems\"\n",
        "  n_samples = y_true.shape[0]\n",
        "  return sum(1.0 if np.array_equal(y_true[i], y_pred[i]) else 0\n",
        "             for i in range(n_samples)\n",
        "            ) / n_samples\n",
        "\n",
        "def jntTestIndClassifiersCV(classifiers, X, matY, clfinstances=None):\n",
        "  if not clfinstances:\n",
        "    _, clfinstances = trainIndClassifiersCV(classifiers, X, matY, return_clfinsts=True)\n",
        "  trnpids = list(map(itemgetter(0), cvsplits))\n",
        "  tstpids = list(map(itemgetter(1), cvsplits))\n",
        "  # list of classifiers that were fitted successfully in the previous experiment\n",
        "  clfnames = sorted(set(x[0] for x in clfinstances))\n",
        "  jclfaccs = []\n",
        "  \n",
        "  for iclf, nclf in enumerate(clfnames):\n",
        "    try:\n",
        "      # when passing the numerical matrix directly to the classifier\n",
        "      _predsst = lambda clf,sids: clf.predict(X[sids,:]).reshape((-1, 1))\n",
        "      # we are now handling the DataFrame directly using pipelines\n",
        "      predsst = lambda clf, sids: clf.predict(X.iloc[sids,:]).reshape((-1, 1))\n",
        "      tstpreds, trnpreds = [], []\n",
        "      for cvid, (trnids,tstids) in enumerate(cvsplits):\n",
        "        indpreds = list(it.starmap(predsst, [(clfinstances[nclf, indY][cvid], \n",
        "                                                tstids)\n",
        "                                           for indY in range(matY.shape[-1])]))\n",
        "        tstpreds.append(np.hstack(indpreds))\n",
        "        indpreds = list(it.starmap(predsst, [(clfinstances[nclf, indY][cvid], \n",
        "                                                trnids)\n",
        "                                           for indY in range(matY.shape[-1])]))\n",
        "        trnpreds.append(np.hstack(indpreds))\n",
        "        \n",
        "      tstaccs = 100*np.array([skmt_mlmc_accuracy_score(matY[sids], preds) \n",
        "                                    for sids, preds in zip(tstpids, tstpreds)])\n",
        "      trnaccs = 100*np.array([skmt_mlmc_accuracy_score(matY[sids], preds)\n",
        "                                    for sids, preds in zip(trnpids, trnpreds)])\n",
        "      clfnfo = [codify_classifier(nclf), \n",
        "                  tstaccs.mean(), trnaccs.mean(), tstaccs.std(), trnaccs.std()]\n",
        "      jclfaccs.append(dict(zip(statnames, clfnfo)))\n",
        "    except (NotFittedError, AttributeError) as err:\n",
        "      pass\n",
        "  return pd.DataFrame(jclfaccs)\n",
        "\n",
        "%time jclfaccs = jntTestIndClassifiersCV(pipelines, rawX, subY_, clfinstances=lclclfs)\n",
        "if not BATCH:\n",
        "  jclfaccs_ = jclfaccs.dropna()\n",
        "  pd_displayHTML(select_clfcombos(jclfaccs_).style.hide_index())\n",
        "  plot_accuracies(jclfaccs_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzGtjWFuDJch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "%time fclfaccs = jntTestIndClassifiersCV(pipelines, rawX, Y_)\n",
        "if not BATCH:\n",
        "  fclfaccs_ = fclfaccs.dropna()\n",
        "  pd_displayHTML(select_clfcombos(fclfaccs_).style.hide_index())\n",
        "  plot_accuracies(fclfaccs_)\n",
        "\"\"\";"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sRiNDJVKLrR6"
      },
      "source": [
        "\n",
        "TODO: \n",
        "\n",
        "*   also pick an optimal encoding scheme\n",
        "*   test some manual feature additions like geographic distances\n",
        "*   also check l1 regularization and see which features matter the most"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJy_udrUzVOL",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameter Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KbptkdD5ka6L",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from functools import reduce \n",
        "\n",
        "paramsKNN = {\n",
        "             'kneighborsclassifier__n_neighbors':range(3, 11), \n",
        "             'kneighborsclassifier__weights':('uniform', 'distance'),\n",
        "             'kneighborsclassifier__algorithm':('auto', 'ball_tree', 'kd_tree', 'brute'),\n",
        "             'kneighborsclassifier__p':(1, 2), \n",
        "            }\n",
        "paramsSVL = {\n",
        "            #'linearsvc__penalty':('l1', 'l2'),\n",
        "            #'linearsvc__loss':('squared_hinge', 'hinge'), \n",
        "            #'linearsvc__dual':(False, True), \n",
        "            #'linearsvc__tol',     \n",
        "             'linearsvc__C':[i/100 for i in range(0, 25)],\n",
        "            #'linearsvc__fit_intercept', \n",
        "            #'linearsvc__intercept_scaling', \n",
        "            #'linearsvc__class_weight':(None, 'balanced'), \n",
        "            #'linearsvc__max_iter', \n",
        "            }\n",
        "paramsSVC = {\n",
        "             'svc__C':10**-np.arange(-1, 2.5, 0.35), \n",
        "             'svc__kernel':('poly', 'rbf', 'sigmoid',), \n",
        "             'svc__degree':range(2, 5), \n",
        "            #'svc__gamma':np.arange(0, 4, 0.5), \n",
        "            #'svc__coef0', \n",
        "            #'svc__shrinking', \n",
        "            #'svc__tol', \n",
        "             'svc__class_weight':(None, 'balanced'), \n",
        "            }\n",
        "paramsMLP = {\n",
        "             'mlpclassifier__hidden_layer_sizes':[(size,) for size in range(50, 60, 20)], \n",
        "             'mlpclassifier__activation':('logistic', 'relu', 'tanh'), \n",
        "             'mlpclassifier__solver':('adam', 'lbfgs'), \n",
        "             'mlpclassifier__alpha':[10.0**i for i in range(-7,4)], \n",
        "             'mlpclassifier__max_iter':range(200, 1001, 100),\n",
        "             'mlpclassifier__early_stopping':(False, True),\n",
        "            }\n",
        "paramsDT  = {\n",
        "             'decisiontreeclassifier__criterion':('gini', 'entropy'), \n",
        "             'decisiontreeclassifier__max_features':('auto', 'sqrt', 'log2', None), \n",
        "             'decisiontreeclassifier__class_weight':(None, 'balanced'),\n",
        "             'decisiontreeclassifier__ccp_alpha':[i/10 for i in range(0, 11)], \n",
        "            #'decisiontreeclassifier__max_depth':, \n",
        "            #'decisiontreeclassifier__max_leaf_nodes':, \n",
        "            #'decisiontreeclassifier__min_impurity_decrease',\n",
        "            #'decisiontreeclassifier__min_impurity_split', \n",
        "            #'decisiontreeclassifier__min_samples_leaf', \n",
        "            #'decisiontreeclassifier__min_samples_split', \n",
        "            #'decisiontreeclassifier__min_weight_fraction_leaf', \n",
        "            #'decisiontreeclassifier__presort', \n",
        "            #'decisiontreeclassifier__random_state', \n",
        "            #'decisiontreeclassifier__splitter'\n",
        "            }\n",
        "paramsRF  = {\n",
        "             'randomforestclassifier__n_estimators':range(10, 200, 10),\n",
        "             'randomforestclassifier__criterion':('entropy', 'gini',),\n",
        "             'randomforestclassifier__max_features':('auto', 'sqrt', 'log2', None),\n",
        "             'randomforestclassifier__bootstrap':(True, False),\n",
        "             'randomforestclassifier__class_weight':(None, 'balanced'), \n",
        "             'randomforestclassifier__ccp_alpha':[i/10 for i in range(1, 2)],\n",
        "            #'randomforestclassifier__max_depth', \n",
        "            #'randomforestclassifier__min_samples_split', \n",
        "            #'randomforestclassifier__min_samples_leaf', \n",
        "            #'randomforestclassifier__min_weight_fraction_leaf',\n",
        "            #'randomforestclassifier__max_leaf_nodes', \n",
        "            #'randomforestclassifier__min_impurity_decrease', \n",
        "            #'randomforestclassifier__min_impurity_split', \n",
        "            #'randomforestclassifier__max_samples', \n",
        "            }\n",
        "\n",
        "paramsDummy = {'dummyclassifier__strategy':('stratified', 'most_frequent', \n",
        "                                           'prior', 'uniform'),\n",
        "              #'dummyclassifier__constant':(1), \n",
        "              #'dummyclassifier__random_state', \n",
        "              }\n",
        "\n",
        "#print(pipelines[('dumbase', 'ohe', 'dim0')].get_params().keys())\n",
        "\n",
        "#grdmclsclf = GridSearchCV(pipelines[clf], param_grid=paramsDummy,\n",
        "#                          cv=cvsplits, n_jobs=2).fit(rawX, subY)\n",
        "lzylength = lambda x: len(x) if hasattr(x, '__len__') else len(list(x))\n",
        "paramcmbs = reduce(lambda x,y: x*y, map(lzylength, paramsDummy.values()))\n",
        "FRAC = 1\n",
        "cmnchoices = set()\n",
        "for clf in (_ for _ in pipelines if _[0] == 'dumbase'):\n",
        "  grdmclsclf = RandomizedSearchCV(pipelines[clf],\n",
        "                                  param_distributions=paramsDummy,\n",
        "                                  n_iter=int(FRAC*paramcmbs),\n",
        "                                  cv=cvsplits, n_jobs=2).fit(rawX, subY)\n",
        "  bstprmsstr = '|'.join(\"{0}={1}\".format(pnme, pval)\n",
        "                        for pnme, pval in grdmclsclf.best_params_.items())\n",
        "  print(codify_classifier(clf), grdmclsclf.best_score_, bstprmsstr)\n",
        "  if len(cmnchoices):\n",
        "    cmnchoices = cmnchoices.intersection(grdmclsclf.best_params_.items())\n",
        "  else:\n",
        "    cmnchoices = set(grdmclsclf.best_params_.items())\n",
        "  grdmclsres = pd.DataFrame(grdmclsclf.cv_results_)\n",
        "print(cmnchoices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "svPQiHs1tukV",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from functools import reduce \n",
        "\n",
        "#print(pipelines[('dumbase', 'ohe', 'dim0')].get_params().keys())\n",
        "\n",
        "#grdmclsclf = GridSearchCV(pipelines[clf], param_grid=paramsDummy,\n",
        "#                          cv=cvsplits, n_jobs=2).fit(rawX, subYmlbl)\n",
        "lzylength = lambda x: len(x) if hasattr(x, '__len__') else len(list(x))\n",
        "paramcmbs = reduce(lambda x,y: x*y, map(lzylength, paramsDummy.values()))\n",
        "FRAC = 1\n",
        "cmnchoices = set()\n",
        "for clf in (_ for _ in pipelines if _[0] == 'dumbase'):\n",
        "  grdmclsclf = RandomizedSearchCV(pipelines[clf],\n",
        "                                  param_distributions=paramsDummy,\n",
        "                                  n_iter=int(FRAC*paramcmbs),\n",
        "                                  cv=cvsplits, n_jobs=2).fit(rawX, subYmlbl)\n",
        "  bstprmsstr = '|'.join(\"{0}={1}\".format(pnme, pval)\n",
        "                        for pnme, pval in grdmclsclf.best_params_.items())\n",
        "  print(codify_classifier(clf), grdmclsclf.best_score_, bstprmsstr)\n",
        "  if len(cmnchoices):\n",
        "    cmnchoices = cmnchoices.intersection(grdmclsclf.best_params_.items())\n",
        "  else:\n",
        "    cmnchoices = set(grdmclsclf.best_params_.items())\n",
        "  grdmclsres = pd.DataFrame(grdmclsclf.cv_results_)\n",
        "print(cmnchoices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVaDCW_HAxgq",
        "colab_type": "text"
      },
      "source": [
        "### Batch experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YdaXBzY6saXL",
        "colab": {}
      },
      "source": [
        "expreport = []\n",
        "try:\n",
        "  if not BATCH:\n",
        "    raise UserWarning(\"Following code in this cell can only be run with BATCH=True from cmd-line\")\n",
        "  CVFOLDS, REPEAT = 2, 5 \n",
        "  samplec, featcsc = alablFull.shape[0], alablFull.shape[1]\n",
        "  subsid = list(range(samplec))  \n",
        "  #params = [(k, 10) for k in range(5, fnccount, 10)]\n",
        "  params = [(CVFOLDS, REPEAT, k) for k in range(5, fnccount, 10)]\n",
        "  ATTEMPTS = 10\n",
        "  #params = list(params)[:1]\n",
        "\n",
        "  # this is to make sure that this block can be run in standalone mode\n",
        "  featsFull_ = featsFull.iloc[subsid,:]\n",
        "  clablFull_ = clablFull.iloc[subsid]\n",
        "  alablFull_ = alablFull.iloc[subsid,:]\n",
        "\n",
        "  X   = featsFull_.copy(deep=False)\n",
        "  ccs = X.select_dtypes(['category']).columns \n",
        "  X[ccs] = X[ccs].apply(lambda x: x.cat.codes)\n",
        "  X  = X.to_numpy()\n",
        "  \n",
        "  Y = clablFull_.copy(deep=False).cat.codes\n",
        "  Y  = Y.to_numpy()\n",
        "\n",
        "  Y_  = alablFull_.copy(deep=False)\n",
        "  ccs = Y_.select_dtypes(['category']).columns\n",
        "  Y_[ccs] = Y_[ccs].apply(lambda x: x.cat.codes)\n",
        "  Y_ = Y_.to_numpy()\n",
        "  \n",
        "  Ymlbl = np.zeros((X.shape[0], alablTabl.shape[0]))\n",
        "  filidx = np.array([ (irow, alablTabl.loc['{0}={1}'.format(fcn, lbl), 'id'])\n",
        "                    for irow, row in enumerate(alablFull_.to_dict(orient='records'))\n",
        "                    for fcn, lbl in row.items() if not pd.isna(lbl)\n",
        "                   ])\n",
        "  Ymlbl[[filidx[:,0], filidx[:,1]]] = 1\n",
        "\n",
        "  for expparam in params:\n",
        "    cvsplits = list(RepeatedKFold(n_splits=expparam[0], \n",
        "                                  n_repeats=expparam[1], random_state=20200408\n",
        "                                 ).split(X, Y))\n",
        "    expr1 = trainFullClassifiersCV(classifiers, X, Y)\n",
        "    # run experiment using X,Y\n",
        "    expreport.extend(dict([('ExpName', 'fulllbl-dense'), \n",
        "                           ('CVF', expparam[0]),\n",
        "                           ('REPEAT', expparam[1]), \n",
        "                           ('K', expparam[2]),\n",
        "                           ('Params', 'default')\n",
        "                          ] + \\\n",
        "                          list(row.items()))\n",
        "                     for row in expr1.to_dict(orient='records')\n",
        "                    ) \n",
        "    expr2 = trainFullClassifiersCV(mlblclasfrs, X, Ymlbl)\n",
        "    expreport.extend(dict([('ExpName', 'fulllbl-sparse'), \n",
        "                           ('CVF', expparam[0]),\n",
        "                           ('REPEAT', expparam[1]), \n",
        "                           ('K', expparam[2]),\n",
        "                           ('Params', 'default')\n",
        "                          ] + \\\n",
        "                          list(row.items()))\n",
        "                     for row in expr2.to_dict(orient='records')\n",
        "                    ) \n",
        "\n",
        "    choices = ncombr(fnccount, expparam[2])\n",
        "    for trial in range(1, ATTEMPTS+1):  #int(FRACP*choices)\n",
        "      subfci = list(sorted(random.sample(range(fnccount), expparam[2])))\n",
        "      subfcs = list(alablFull.columns[i] for i in subfci)\n",
        "      \n",
        "      alablSub_  = alablFull.iloc[subsid,subfci]\n",
        "\n",
        "      clablSub = ['|'.join('{0}={1}'.format(k,v)\n",
        "                           for k,v in row.items() if not pd.isna(v))\n",
        "                  for row in alablSub_.to_dict(orient='records')\n",
        "                 ]\n",
        "      clablSub = pd.Series(clablSub, name=header[-1])\n",
        "      subY = clablSub.astype('category').cat.codes\n",
        "      subY = subY.to_numpy()\n",
        "\n",
        "      subY_ = alablSub_.copy(deep=False)\n",
        "      ccs = subY_.select_dtypes(['category']).columns\n",
        "      subY_[ccs] = subY_[ccs].apply(lambda x: x.cat.codes)\n",
        "      subY_ = subY_.to_numpy()\n",
        "\n",
        "      subYmlbl = np.zeros((Y.shape[0], alablTabl.shape[0]))\n",
        "      filidx = np.array([ (irow, alablTabl.loc['{0}={1}'.format(fcn, lbl), 'id'])\n",
        "                         for irow, row in enumerate(alablSub_.to_dict(orient='records'))\n",
        "                         for fcn, lbl in row.items() if not pd.isna(lbl)\n",
        "                       ])\n",
        "      subYmlbl[[filidx[:,0], filidx[:,1]]] = 1\n",
        "      \n",
        "      expr1 = trainFullClassifiersCV(classifiers, X, subY)\n",
        "      expreport.extend(dict([('ExpName', 'sublbl-dense'), ('CVF', expparam[0]),\n",
        "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
        "                             ('TRIAL', 'T{0}'.format(trial)), \n",
        "                             ('Params', 'default')\n",
        "                            ] + \\\n",
        "                            list(row.items()))\n",
        "                       for row in expr1.to_dict(orient='records')) \n",
        "      \n",
        "      expr2 = trainFullClassifiersCV(mlblclasfrs, X, subYmlbl)\n",
        "      expreport.extend(dict([('ExpName', 'sublbl-sparse'), ('CVF', expparam[0]),\n",
        "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
        "                             ('TRIAL', 'T{0}'.format(trial)), \n",
        "                             ('Params', 'default')\n",
        "                            ] + \\\n",
        "                            list(row.items()))\n",
        "                       for row in expr2.to_dict(orient='records')) \n",
        "      \n",
        "      expr3, clfs = trainIndClassifiersCV(classifiers, X, subY_, return_clfinsts=True)\n",
        "      expreport.extend(dict([('ExpName', 'sublbl-dense-ind'), ('CVF', expparam[0]),\n",
        "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
        "                             ('TRIAL', 'T{0}'.format(trial)),\n",
        "                             ('Params', 'default')\n",
        "                            ] + \\\n",
        "                            list(row.items()))\n",
        "                       for row in expr3.to_dict(orient='records')) \n",
        "      \n",
        "      expr4 = jntTestIndClassifiersCV(classifiers, X, subY_, clfs)\n",
        "      expreport.extend(dict([('ExpName', 'sublbl-dense-jnt'), ('CVF', expparam[0]),\n",
        "                             ('REPEAT', expparam[1]), ('K', expparam[2]),\n",
        "                             ('TRIAL', 'T{0}'.format(trial)),\n",
        "                             ('Params', 'default')\n",
        "                            ] + \\\n",
        "                            list(row.items()))\n",
        "                       for row in expr4.to_dict(orient='records')) \n",
        "  \n",
        "  pd.DataFrame(expreport).to_html('sigtyp-st2020-part1-batchexps-results.html', index=False)\n",
        "  pd.DataFrame(expreport).to_json('sigtyp-st2020-part1-batchexps-results.json')\n",
        "except KeyboardInterrupt:\n",
        "  pd.DataFrame(expreport).to_html('sigtyp-st2020-part1-batchexps-results-aborted.html', index=False)\n",
        "  pd.DataFrame(expreport).to_json('sigtyp-st2020-part1-batchexps-results-aborted.json')\n",
        "except UserWarning as err:\n",
        "  print(err)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}